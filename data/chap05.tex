% !TeX root = ../thuthesis-example.tex

\chapter{总结与展望}

\section{本文工作总结}

本文主要研究了在模型微调场景下批标准化BatchNorm存在的缺陷与不足，并基于双分支结构和随机前向传播的思想，针对模型微调场景设计了新的标准化层结构，随机标准化层StochNorm。

本文在三个细粒度分类数据集上验证了StochNorm的性能，证明了在数据量较少的模型微调场景下，StochNorm具有超过主流的基于正则化约束的模型微调方法的性能。同时本文还将StochNorm与多种不同的深度
神经网络结构以及模型微调方法进行了结合，进一步取得了效果上的提升，从而也展示了StochNorm作为一种网络结构层面方法的通用性。

本文还展示了StochNorm在两个实际场景，医疗影像诊断和风速预测中的应用。通过在现有模型中加入StochNorm技术，能够获得一定的效果提升。最后本文还基于迁移学习算法库的平台，对StochNorm进行了算法
集成，提供了方便快捷调用的API接口。

\section{未来工作展望}

\textbf{标准化技术的发展 } 本文在提出StochNorm的同时，也对目前广泛使用的BatchNorm中存在的问题进行了一定的分析。尽管类似BatchNorm的常用的标准化层结构已经在各领域的各类任务中取得了喜人的成果，
其中存在的很多不足与缺陷也不能忽略。同时，许多标准化技术的作用原理仍然没有得到解释，实验性的结果也需要扎实的理论分析来肯定。相信今后也会有越来越多的研究人员开始对这些经典的方法进行
深入的分析与改进，提出更泛用更有效的标准化技术。

\textbf{随机性在深度学习中的应用 } 本文提出的StochNorm的一大创新点便是将随机性引入了标准化层结构，实现了一种隐式的结构层面正则化约束。随机性对于深度学习而言有着非常重要的意义，从神经
元随机失活（Dropout）到输入数据的随机增广（Augmentation），随机性的引入为深度学习带来了更大的可能性。如今很多新的研究工作也通过掩码技术、对比学习等方式将随机性从不同的角度引入深度学习
当中，相信未来也能看到更多随机性在深度学习中的应用。

\textbf{挖掘预训练模型的潜力 } 
目前模型微调领域的工作往往都会人为地区分为预训练阶段和微调阶段。但是预训练与微调本应是有机的整体，人为地进行区分就会经常出现预训练模型与下游任务差异过大，带来负收益的现象。如今有越来越多的研究人员
开始重视这一观点，诸如针对微调任务的特征利用元学习（Meta Learning）的技术设计预训练模型，获得对下游任务更有意义的预训练模型。

同时，预训练模型中包含的知识也并非只有可学习的网络参数。本文提出的StochNorm中，通过双分支的结构，巧妙地将预训练模型中以往被忽视的统计类参数引入了模型微调阶段，进一步挖掘了预训练模型
中的知识。如今也有很多新的工作开始重视预训练模型中可学习的网络参数之外的信息，包括深度神经网络优化器、预训练使用的数据增广甚至预训练阶段的的梯度信息等，从更多的维度对预训练模型进行分析，
相信未来能看到对预训练模型中潜力的充分挖掘。


\textbf{更多的应用场景 } 除了本文提到的两个应用场景外，深度学习还有很广泛的应用场景，诸如目标检测、实例分割、关键点检测等多种任务。在这些任务中，也依然存在着数据量不足的问题，如何更好地
利用现有的知识来提升训练效果，以及如何避免数据量不足带来的问题，都是值得进一步研究的问题。相信今后的研究工作一定能在这些领域获得突破。
